# -*- coding: utf-8 -*-
"""Tugas NLP 1-4-2022

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IBawaU1YSGig_OuVZ6ABbRxfffqae6mV

Nama : Makrufiah Sakatri |
Kelas : Alphago
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

# %matplotlib inline

!pip -q install sastrawi

import nltk
nltk.download('stopwords')

"""##Akuisisi Data

link drive = https://drive.google.com/file/d/1s5u0S2JtB2mT8-pvVXR9tmCYGfH7JvRm/view?usp=sharing
"""

data = pd.read_csv('data3.csv') #data sudah dikurangi pake jupyter notebook, biar uploadnya ga lama:)
data

data.info()

data.sort_values('rating',inplace=False)
data.sort_values('rating',ignore_index=True)
data

data['rating'].value_counts()

print('Total Jumlah SMS:', data.shape[0], 'data\n')
print('terdiri dari (label):')
print('-- [0] Penilaian Sangat Buruk\t:', data[data.rating == 1 ].shape[0], 'data')
print('-- [1] Penilaian Buruk\t\t:', data[data.rating == 2 ].shape[0], 'data')
print('-- [2] Penilaian Cukup\t\t:', data[data.rating == 3].shape[0], 'data')
print('-- [3] Penilaian Baik\t\t:', data[data.rating == 4].shape[0], 'data')
print('-- [4] Penilaian Sangat Baik\t:', data[data.rating == 5].shape[0], 'data')

height = data['rating'].value_counts()
labels = ('Penilaian Sangat Baik', 'Penilaian Baik', 'Penilaian Sangat Buruk', 'Penilaian Cukup', 'Penilaian Buruk')
y_pos = np.arange(len(labels))

plt.figure(figsize=(10,4), dpi=80)
plt.ylim(0,3000)
plt.title('Distribusi Kategori SMS', fontweight='bold')
plt.xlabel('Kategori', fontweight='bold')
plt.ylabel('Jumlah', fontweight='bold')
plt.bar(y_pos, height, color=['deepskyblue', 'royalblue', 'skyblue', 'royalblue', 'skyblue'])
plt.xticks(y_pos, labels)
plt.show()

data['reviewContent'].unique()



"""# Text Preprocessing

## Case Folding
"""

import re

# Buat fungsi untuk langkah case folding
def casefolding(text):
  text = text.lower()                               # Mengubah teks menjadi lower case
  text = re.sub(r'https?://\S+|www\.\S+', '', text) # Menghapus URL
  text = re.sub(r'[-+]?[0-9]+', '', text)           # Menghapus angka
  text = re.sub(r'[^\w\s]','', text)                # Menghapus karakter tanda baca
  text = text.strip()
  return text

raw_sample = data['reviewContent'].iloc[5]
case_folding = casefolding(raw_sample)

print('Raw data\t: ', raw_sample)
print('Case folding\t: ', case_folding)

"""## Word Normalization"""

# Download corpus singkatan
!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv

key_norm = pd.read_csv('key_norm.csv')

def text_normalize(text):
  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])
  text = str.lower(text)
  return text



"""#Stemming"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Buat fungsi untuk langkah stemming bahasa Indonesia
def stemming(text):
  text = stemmer.stem(text)
  return text

raw_sample = data['reviewContent'].iloc[33]
case_folding = casefolding(raw_sample)
text_stemming = stemming(case_folding)

print('Raw data\t\t: ', raw_sample)
print('Case folding\t\t: ', case_folding)
print('Stemming\t\t: ', text_stemming)

"""## Text Preprocessing Pipeline"""

# Buat fungsi untuk menggabungkan seluruh langkah text preprocessing
def text_preprocessing_process(text):
  text = casefolding(text)
  text = text_normalize(text)
  text = stemming(text)
  return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data['clean_teks'] = data['reviewContent'].apply(text_preprocessing_process)
# 
# # Perhatikan waktu komputasi ketika proses text preprocessing

data

# Simpan data yang telah melalui text preprocessing agar kita tidak perlu menjalankan proses tersebut mulai awal (Opsional)
data.to_csv('clean_data3.csv')

"""# Feature Engineering"""

# Pisahkan kolom feature dan target
X = data['clean_teks']
y = data['reviewContent']

X

y

"""## Feature Extraction (Bag of Words & N-Gram)
Proses mengubah teks menjadi vektor menggunakan metode BoW
"""

'''
Convert a collection of text documents to a matrix of token counts.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
'''
from sklearn.feature_extraction.text import CountVectorizer

# BoW - Unigram
vec = CountVectorizer(ngram_range=(2,2))
vec.fit(X)

# Melihat Jumlah Fitur
print(len(vec.get_feature_names_out()))

# Melihat fitur-fitur apa saja yang ada di dalam corpus
print(vec.get_feature_names_out())

# Melihat matriks jumlah token
# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)

X_unigram = vec.transform(X).toarray()

X_unigram

data_unigram = pd.DataFrame(X_unigram, columns=vec.get_feature_names_out())
data_unigram

with open('bow.pickle', 'wb') as output:
  pickle.dump(X_unigram, output)

"""## Feature Extraction (TF-IDF & N-Gram)
Proses mengubah teks menjadi vector menggunakan metode TF-IDF
"""

'''
Convert a collection of raw documents to a matrix of TF-IDF features
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
'''
from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(ngram_range=(2,2))
tf_idf.fit(X)

X_tf_idf = tf_idf.transform(X)

# Melihat Jumlah Fitur
print(len(tf_idf.get_feature_names_out()))

# Melihat fitur-fitur apa saja yang ada di dalam corpus
print(tf_idf.get_feature_names_out())

# Melihat matriks jumlah token menggunakan TF IDF, lihat perbedaannya dengan metode BoW
# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)

X_tf_idf = tf_idf.transform(X).toarray()

X_tf_idf

data_tf_idf = pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())
data_tf_idf

with open('tf_idf.pickle', 'wb') as output:
  pickle.dump(X_tf_idf, output)

"""## Feature Selection"""

# Mengubah nilai data tabular tf-idf menjadi array agar dapat dijalankan pada proses seleksi fitur
X = np.array(data_tf_idf)
y = np.array(y)

'''
Select features according to the k highest scores.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html

Compute chi-squared stats between each non-negative feature and class.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html
'''

from sklearn.feature_selection import SelectKBest 
from sklearn.feature_selection import chi2 

# Ten features with highest chi-squared statistics are selected 
chi2_features = SelectKBest(chi2, k=5000) 
X_kbest_features = chi2_features.fit_transform(X, y) 
  
# Reduced features 
print('Original feature number:', X.shape[1]) 
print('Reduced feature number:', X_kbest_features.shape[1])

# chi2_features.scores_ adalah nilai chi-square, semakin tinggi nilainya maka semakin baik fiturnya
data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])
data_chi2

# Menampilkan fitur beserta nilainya
feature = tf_idf.get_feature_names_out()
data_chi2['fitur'] = feature
data_chi2

# Mengurutkan fitur terbaik
data_chi2.sort_values(by='nilai', ascending=False)

# Menampilkan mask pada feature yang diseleksi
# False berarti fitur tidak terpilih dan True berarti fitur terpilih
mask = chi2_features.get_support()
mask

# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square
new_feature = []
for bool, f in zip(mask, feature):
  if bool:
    new_feature.append(f)
  selected_feature = new_feature
selected_feature

# Menampilkan fitur-fitur yang sudah diseleksi 
# Beserta nilai vektornya pada keseluruhan data untuk dijalankan pada proses machine learning

# Hanya k fitur yang terpilih sesuai parameter k yang ditentukan sebelumnya

data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)
data_selected_feature

with open('best_feature.pickle', 'wb') as output:
  pickle.dump(X_kbest_features, output)

"""# WordCloud"""

# Import Library WordCloud. WordCloud digunakan untuk melihat secara visual kata-kata yang paling sering muncul.
# Import Library cv2 untuk mengolah gambar menjadi masking WordCloud

import cv2
from wordcloud import WordCloud

# Download gambar masking
!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/img/cloud.jpg

originalImage = cv2.imread('cloud.jpg')
grayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2GRAY)
(thresh, cloud_mask) = cv2.threshold(grayImage, 100, 255, cv2.THRESH_BINARY)

# Tampilkan masking
from google.colab.patches import cv2_imshow

cv2_imshow(cloud_mask)

# WordCloud Label Penilaian Sangat Baik
PSB = data[data.rating == 5]
normal_string = []

for t in PSB.reviewContent:
  normal_string.append(t)

normal_string = pd.Series(normal_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(normal_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label Penilaian Baik

PB = data[data.rating == 4]
fraud_string = []

for t in PB.reviewContent:
  fraud_string.append(t)

fraud_string = pd.Series(fraud_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(fraud_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label Penilaian Cukup

PC = data[data.rating == 3]
promo_string = []

for t in PC.reviewContent:
  promo_string.append(t)

promo_string = pd.Series(promo_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(promo_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label Penilaian Buruk

PB= data[data.rating == 2]
promo_string = []

for t in PB.reviewContent:
  promo_string.append(t)

promo_string = pd.Series(promo_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(promo_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label Penilaian Sangat Buruk

PSB= data[data.rating == 1]
promo_string = []

for t in PB.reviewContent:
  promo_string.append(t)

promo_string = pd.Series(promo_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(promo_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""# Student Activity
Lakukan proses di atas menggunakan dataset review product https://drive.google.com/file/d/1qn5WXp-H95_FL_Rx5oqvfZaflYdHsnrF/view?usp=sharing

Tugas Anda:
- Tentukan langkah pre-processing yang tepat untuk dataset di atas.
- Gunakan range `n_gram` yang berbeda. Amati apa perbedaannya.
- Menurut Anda, apakah `term` yang dihasilkan (`X_kbest_features`) pada feature selection sudah memiliki informasi yang relevan?

Setelah dikerjakan, buatlah resume berdasarkan pengalaman Anda dalam melakukan pre-processing dan feature engineering.

Kumpulkan tugas Anda pada: https://s.id/tugas-nlp-ofa


"""



"""##Jawaban/Resume

1. Pada dataset tersebut sesuai percobaan diatas, prepocessing yang cocok digunakan adalah Case Folding dan Stemming. Tidak cocok mengunakan stop removal word karena akan mengubah makna reviewContent pada kategori penilaian yang lain.
2. Saat n_gram diubah menjadi (2,2) (bigram) dihasilkan jumlah fitur semakin banyak dibandingkan menggunakan (1,1) (unigram). Mengartikan nilai n_gram akan memperbanyak fitur.
3. Sudah dapat dilihat nilai fitur tertinggi pada kata yang sering digunakan (pada penilaian sangat baik dan memiliki jumlah rating terbanyak) dan sebaliknya.
"""